{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"census-income\").config(\"spark-master\", \"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up schema and read data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "struct_fields_list = [\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"class_of_worker\", StringType(), True),\n",
    "    StructField(\"industry_code\", StringType(), True),\n",
    "    StructField(\"occupation_code\", StringType(), True),\n",
    "    StructField(\"education\", StringType(), True),\n",
    "    StructField(\"wage_per_hr\", DoubleType(), True),\n",
    "    StructField(\"enrolled_in_edu_inst_last_wk\", StringType(), True),\n",
    "    StructField(\"marital_status\", StringType(), True),\n",
    "    StructField(\"major_industry_code\", StringType(), True),\n",
    "    StructField(\"major_occupation_code\", StringType(), True),\n",
    "    StructField(\"race\", StringType(), True),\n",
    "    StructField(\"hispanic_origin\", StringType(), True),\n",
    "    StructField(\"sex\", StringType(), True),\n",
    "    StructField(\"mem_labour_union\", StringType(), True),\n",
    "    StructField(\"unemployment_reason\", StringType(), True),\n",
    "    StructField(\"employment_status\", StringType(), True),\n",
    "    StructField(\"capital_gain\", DoubleType(), True),\n",
    "    StructField(\"capital_loss\", DoubleType(), True),\n",
    "    StructField(\"stock_dividends\", DoubleType(), True),\n",
    "    StructField(\"tax_filer_status\", StringType(), True),\n",
    "    StructField(\"prev_region\", StringType(), True),\n",
    "    StructField(\"prev_state\", StringType(), True),\n",
    "    StructField(\"household_status\", StringType(), True),\n",
    "    StructField(\"household_summary\", StringType(), True),\n",
    "    StructField(\"instance_weight\", DoubleType(), True),\n",
    "    StructField(\"migration_code_msa\", StringType(), True),\n",
    "    StructField(\"migration_code_region\", StringType(), True),\n",
    "    StructField(\"migration_code_within_region\", StringType(), True),\n",
    "    StructField(\"live_in_this_house_one_year_ago\", StringType(), True),\n",
    "    StructField(\"migration_prev_res_in_sunbelt\", StringType(), True),\n",
    "    StructField(\"num_persons_for_employer\", IntegerType(), True),\n",
    "    StructField(\"parent\", StringType(), True),\n",
    "    StructField(\"birth_country_father\", StringType(), True),\n",
    "    StructField(\"birth_country_mother\", StringType(), True),\n",
    "    StructField(\"birth_country_self\", StringType(), True),\n",
    "    StructField(\"citizenship\", StringType(), True),\n",
    "    StructField(\"own_business\", StringType(), True),\n",
    "    StructField(\"veteran_QA\", StringType(), True),\n",
    "    StructField(\"veteran_benefits\", StringType(), True),\n",
    "    StructField(\"weeks_worked_in_yr\", IntegerType(), True),\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"income\", StringType(), True),\n",
    "]\n",
    "\n",
    "schema = StructType(struct_fields_list)\n",
    "\n",
    "# read data, remove trailing and leading whitespace, set null value to ?\n",
    "spark_train = spark.read.csv(\"census-income.data\", \n",
    "                             schema=schema, \n",
    "                             ignoreLeadingWhiteSpace=True,\n",
    "                             ignoreTrailingWhiteSpace=True,\n",
    "                             nullValue=\"?\")\n",
    "spark_test = spark.read.csv(\"census-income.test\", \n",
    "                             schema=schema, \n",
    "                             ignoreLeadingWhiteSpace=True,\n",
    "                             ignoreTrailingWhiteSpace=True,\n",
    "                             nullValue=\"?\")\n",
    "\n",
    "TRAIN_SIZE = spark_train.count()\n",
    "TEST_SIZE = spark_test.count()\n",
    "\n",
    "print(\"Train set shape: ({},{})\".format(TRAIN_SIZE, len(spark_train.columns)))\n",
    "print(\"Test set shape: ({}, {})\".format(TEST_SIZE, len(spark_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full dataset \n",
    "spark_ds = spark_train.union(spark_test)\n",
    "# Drop instance_weight column (according to dataset description)\n",
    "spark_ds = spark_ds.drop(\"instance_weight\")\n",
    "DATASET_SIZE = spark_ds.count()\n",
    "# Full dataset shape\n",
    "print(\"Dataset shape: ({}, {})\".format(DATASET_SIZE, len(spark_ds.columns)))\n",
    "print()\n",
    "# Print first 5 rows\n",
    "spark_ds.show(5, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"income\"\n",
    "nominal_cols = [x[0] for x in spark_ds.dtypes if x[1] == \"string\" and x[0] != target_col]\n",
    "numeric_cols = [x[0] for x in spark_ds.dtypes if x[1] != \"string\" and x[0] != target_col]\n",
    "\n",
    "print(\"Nominal columns:\", nominal_cols)\n",
    "print(\"There are {} nominal columns.\".format(len(nominal_cols)))\n",
    "print()\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"There are {} numeric columns.\".format(len(numeric_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty values preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Count null and empty values in each columns\n",
    "# In numeric columns\n",
    "print(\"Empty values percentage in numeric columns:\")\n",
    "spark_ds.select([(count(when((col(c).isNull()) | (col(c) == \"\"), c)) / DATASET_SIZE * 100)\\\n",
    "                 .alias(c) for c in numeric_cols]).show(vertical=True)\n",
    "# In nominal columns\n",
    "print(\"Empty values percentage in nominal columns:\")\n",
    "spark_ds.select([(count(when((col(c).isNull()) | (col(c) == \"\"), c)) / DATASET_SIZE * 100)\\\n",
    "                 .alias(c) for c in nominal_cols]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all numeric columns do not have empty values. Let's inspect nominal columns which have empty values and process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect columns with empty counts\n",
    "cols_with_empty_vals = [\"prev_state\", \"migration_code_msa\", \"migration_code_region\", \"migration_code_within_region\",\n",
    "                       \"migration_prev_res_in_sunbelt\", \"birth_country_father\", \"birth_country_mother\", \"birth_country_self\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the output above that 4 columns (which start with migration_code*) have a really high percentage of missing values compared to all other columns (around 50% data missing). We will drop these columns because it might be misleading to include these columns in our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Notice that 4 columns: migration_code_* have a really high percentage of null values, ~50%\n",
    "# Inspect a sample migration_code_* column's unique values count\n",
    "print(\"Unique values count of migration_code_msa column:\")\n",
    "spark_ds.groupBy(\"migration_code_msa\")\\\n",
    "        .count()\\\n",
    "        .withColumn(\"count\", col(\"count\") / DATASET_SIZE * 100)\\\n",
    "        .orderBy(desc(\"count\")).show()\n",
    "\n",
    "# We will drop these columns\n",
    "redundant_cols = [\"migration_code_msa\", \"migration_code_region\", \"migration_code_within_region\",\n",
    "                         \"migration_prev_res_in_sunbelt\"]\n",
    "\n",
    "spark_ds = spark_ds.drop(*redundant_cols)\n",
    "cols_with_empty_vals = [c for c in cols_with_empty_vals if c not in redundant_cols]\n",
    "nominal_cols = [c for c in nominal_cols if c not in redundant_cols]\n",
    "\n",
    "print(\"Total number of columns after dropping:\", len(spark_ds.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to get unique values percentage in each column\n",
    "def get_unique_values_percentage(col_name):\n",
    "    return spark_ds.groupBy(col_name)\\\n",
    "                    .count()\\\n",
    "                    .withColumn(\"count\", col(\"count\") / DATASET_SIZE * 100)\\\n",
    "                    .orderBy(desc(\"count\"))\n",
    "\n",
    "# helper function to get most frequent value from unique value counts list\n",
    "def get_most_freq_value(unique_counts, col_name):\n",
    "    return unique_counts\\\n",
    "            .orderBy(desc(\"count\"))\\\n",
    "            .select(col_name)\\\n",
    "            .collect()[0][col_name]\n",
    "\n",
    "# helper function to replace null values in column with new specified value\n",
    "def replace_nulls(col_name, new_value):\n",
    "    return spark_ds.withColumn(col_name, \\\n",
    "         when(col(col_name).isNull(), new_value).otherwise(col(col_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For remaining columns with lower percentage of null values, our strategy is to replace null values with the most frequent value in each corresponding column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal wih remaining columns with empty values but smaller percentage\n",
    "prev_state_count = get_unique_values_percentage(\"prev_state\")\n",
    "\n",
    "birth_country_father_count = get_unique_values_percentage(\"birth_country_father\")\n",
    "\n",
    "birth_country_mother_count = get_unique_values_percentage(\"birth_country_mother\")\n",
    "\n",
    "birth_country_self_count = get_unique_values_percentage(\"birth_country_self\")\n",
    "\n",
    "print(\"prev_state column unique values count:\")\n",
    "prev_state_count.show()\n",
    "print(\"birth_country_father column unique values count:\")\n",
    "birth_country_father_count.show()\n",
    "print(\"birth_country_mother column unique values count:\")\n",
    "birth_country_mother_count.show()\n",
    "print(\"birth_country_self column unique values count:\")\n",
    "birth_country_self_count.show()\n",
    "\n",
    "# We will replace null values with the most frequent value in each column\n",
    "prev_state_most_freq = get_most_freq_value(prev_state_count, \"prev_state\")\n",
    "print(\"Most frequent value in prev_state column:\", prev_state_most_freq)\n",
    "\n",
    "birth_country_father_most_freq = get_most_freq_value(birth_country_father_count, \"birth_country_father\")\n",
    "print(\"Most frequent value in birth_country_father column:\", birth_country_father_most_freq)\n",
    "\n",
    "birth_country_mother_most_freq = get_most_freq_value(birth_country_mother_count, \"birth_country_mother\")\n",
    "print(\"Most frequent value in birth_country_mother column:\", birth_country_mother_most_freq)\n",
    "\n",
    "birth_country_self_most_freq = get_most_freq_value(birth_country_self_count, \"birth_country_self\")\n",
    "print(\"Most frequent value in birth_country_self column:\", birth_country_self_most_freq)\n",
    "\n",
    "# Replace null values\n",
    "spark_ds = replace_nulls(\"prev_state\", prev_state_most_freq)\n",
    "spark_ds = replace_nulls(\"birth_country_father\", birth_country_father_most_freq)\n",
    "spark_ds = replace_nulls(\"birth_country_mother\", birth_country_mother_most_freq)\n",
    "spark_ds = replace_nulls(\"birth_country_self\", birth_country_self_most_freq)\n",
    "\n",
    "print(\"Verify results:\")\n",
    "# verify results\n",
    "get_unique_values_percentage(\"prev_state\").show()\n",
    "get_unique_values_percentage(\"birth_country_father\").show()\n",
    "get_unique_values_percentage(\"birth_country_mother\").show()\n",
    "get_unique_values_percentage(\"birth_country_self\").show()\n",
    "print(\"Empty values count:\")\n",
    "spark_ds.select([(count(when((col(c).isNull()) | (col(c) == \"\"), c)) / DATASET_SIZE * 100)\\\n",
    "                 .alias(c) for c in cols_with_empty_vals]).show(vertical=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some inspection, we see that some columns have a really high percentage of \"Not in universe\" values, which means the value recorded is not in the survey's values domain. We probably do not want to include these columns when we train our classifier. We set the threshold to be 90% (i.e. if the percentage of \"Not in universe\" values is more than 90% in a specific column, we will drop it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show \"Not in universe\" percentage count in each column\n",
    "spark_ds.select([(count(when(col(c) == \"Not in universe\", c)) / DATASET_SIZE * 100)\\\n",
    "                 .alias(c) for c in nominal_cols]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these 6 columns have more than 90% of \"Not in universe\" values. We will drop these columns from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_niu_cols = ['enrolled_in_edu_inst_last_wk', 'mem_labour_union', 'unemployment_reason', 'prev_region', 'prev_state', 'veteran_QA']\n",
    "# drop these columns\n",
    "spark_ds = spark_ds.drop(*high_niu_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final dataset shape: ({}, {})\".format(DATASET_SIZE, len(spark_ds.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert target variable (income) for binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show percentage of each unique values in our target variable\n",
    "income_count = get_unique_values_percentage(\"income\")\n",
    "income_count.show()\n",
    "# We could see that around 94% of our data belongs to -50000 class (below 50$K dollar income/year)\n",
    "# This signifies a binary classification problem with imbalanced data\n",
    "# We will convert major label to be 0 (negative) and minor label to be 1 (positive)\n",
    "major_label = income_count.orderBy(desc(\"count\"))\\\n",
    "                .select(\"income\")\\\n",
    "                .collect()[0][\"income\"]\n",
    "minor_label = income_count.orderBy(desc(\"count\"))\\\n",
    "                .select(\"income\")\\\n",
    "                .collect()[1][\"income\"]\n",
    "\n",
    "print(\"Major label:\", major_label)\n",
    "print(\"Minor label:\", minor_label)\n",
    "\n",
    "# Convert to numeric\n",
    "spark_ds = spark_ds.withColumn(\"income\", \\\n",
    "         when(col(\"income\") == minor_label, 1).otherwise(0))\n",
    "# Convert income column to be numeric type (int)\n",
    "spark_ds = spark_ds.withColumn(\"income\", col(\"income\").cast(\"int\"))\n",
    "\n",
    "# verify results\n",
    "print(\"Verify results:\")\n",
    "get_unique_values_percentage(\"income\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"income\"\n",
    "nominal_cols = [x[0] for x in spark_ds.dtypes if x[1] == \"string\" and x[0] != target_col]\n",
    "numeric_cols = [x[0] for x in spark_ds.dtypes if x[1] != \"string\" and x[0] != target_col]\n",
    "\n",
    "print(\"Nominal columns:\", nominal_cols)\n",
    "print(\"There are {} nominal columns.\".format(len(nominal_cols)))\n",
    "print()\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"There are {} numeric columns.\".format(len(numeric_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore numeric columns first, then move on to nominal columns. We will also try to only show columns with meaningful data because the total number of columns might be too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_ds.select(\"age\").describe().show()\n",
    "median_age = spark_ds.approxQuantile(\"age\", [0.5], 0)[0]\n",
    "print(\"Median age:\", median_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "We will plot histogram labeled by target column (i.e. income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_0 = spark_ds.where(col(\"income\") == 0).select(\"age\").rdd.flatMap(lambda x: x).collect()\n",
    "age_1 = spark_ds.where(col(\"income\") == 1).select(\"age\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(age_0, bins=50, label=\"Below $50K income\")\n",
    "plt.hist(age_1, bins=50, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Histogram of Age\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that people who have above \\\\$50K income are more than 20 years old (age > 20).\n",
    "- Most of people who have > \\\\$50K+ income are from ~30 - ~60 years old.\n",
    "- The data distribution is slightly right-skewed. You can see that from the point of age 50 onwards, the frequency column's height is almost half.\n",
    "- More younger people (age < 50) takes part in the survey (census) than older people (age > 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wage/hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dollars first (original data is in cents unit)\n",
    "converted_wage_per_hr = spark_ds.select((col(\"wage_per_hr\") / 100).alias(\"wage_per_hour ($)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_wage_per_hr.describe().show()\n",
    "median_wage_per_hr = converted_wage_per_hr.approxQuantile(\"wage_per_hour ($)\", [0.5], 0)[0]\n",
    "print(\"Median wage/hr:\", median_wage_per_hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wage_per_hr_0 = spark_ds.where(col(\"income\") == 0).select(col(\"wage_per_hr\") / 100).rdd.flatMap(lambda x: x).collect()\n",
    "wage_per_hr_1 = spark_ds.where(col(\"income\") == 1).select(col(\"wage_per_hr\") / 100).rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(wage_per_hr_0, bins=50, label=\"Below $50K income\")\n",
    "plt.hist(wage_per_hr_1, bins=50, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Histogram of wage/hr\")\n",
    "plt.xlabel('Wage/hr')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data distribution is extremely right skewed. Most of the data's wage per hour is 0, meaning most of the people doing the survey does not have a job. We can also see that most of the people who have low wage/hr (below \\\\$20/hr) have below \\\\$50K+ income/yr.\n",
    "- Interestingly, most of people who have above \\$50k+ income (orange part), have wage/hr equals 0. This might mean they have a different source of income other than working or maybe the data might be corrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital loss, capital gain and stock dividends\n",
    "Like wage/hr column above, these columns also have extremely right-skewed data distribution, i.e. most of the data have value 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_ds.select(\"capital_loss\", \"capital_gain\", \"stock_dividends\").describe().show()\n",
    "median_capital_loss = spark_ds.approxQuantile(\"capital_loss\", [0.5], 0)[0]\n",
    "median_capital_gain = spark_ds.approxQuantile(\"capital_gain\", [0.5], 0)[0]\n",
    "median_stock_dividends = spark_ds.approxQuantile(\"stock_dividends\", [0.5], 0)[0]\n",
    "print(\"Median capital loss:\", median_capital_loss)\n",
    "print(\"Median capital gain:\", median_capital_gain)\n",
    "print(\"Median capital stock_dividends:\", median_stock_dividends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_loss_0 = spark_ds.where(col(\"income\") == 0).select(\"capital_loss\").rdd.flatMap(lambda x: x).collect()\n",
    "capital_loss_1 = spark_ds.where(col(\"income\") == 1).select(\"capital_loss\").rdd.flatMap(lambda x: x).collect()\n",
    "capital_gain_0 = spark_ds.where(col(\"income\") == 0).select(\"capital_gain\").rdd.flatMap(lambda x: x).collect()\n",
    "capital_gain_1 = spark_ds.where(col(\"income\") == 1).select(\"capital_gain\").rdd.flatMap(lambda x: x).collect()\n",
    "stock_0 = spark_ds.where(col(\"income\") == 0).select(\"stock_dividends\").rdd.flatMap(lambda x: x).collect()\n",
    "stock_1 = spark_ds.where(col(\"income\") == 1).select(\"stock_dividends\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "# Capital loss histogram\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(capital_loss_0, bins=50, label=\"Below $50K income\")\n",
    "plt.hist(capital_loss_1, bins=50, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Histogram of Capital loss\")\n",
    "plt.xlabel('Capital loss')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Capital gain histogram\n",
    "plt.subplot(2,2,2)\n",
    "plt.hist(capital_gain_0, bins=50, label=\"Below $50K income\")\n",
    "plt.hist(capital_gain_1, bins=50, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Histogram of Capital gain\")\n",
    "plt.xlabel('Capital gain')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Stock dividends histogram\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(stock_0, bins=50, label=\"Below $50K income\")\n",
    "plt.hist(stock_1, bins=50, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Histogram of Stock dividends\")\n",
    "plt.xlabel('Stock dividends')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weeks worked in year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_ds.select(\"weeks_worked_in_yr\").describe().show()\n",
    "median_weeks_worked_in_yr = spark_ds.approxQuantile(\"weeks_worked_in_yr\", [0.5], 0)[0]\n",
    "print(\"Median weeks worked in a year:\", median_weeks_worked_in_yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_worked_0 = spark_ds.where(col(\"income\") == 0).select(\"weeks_worked_in_yr\").rdd.flatMap(lambda x: x).collect()\n",
    "weeks_worked_1 = spark_ds.where(col(\"income\") == 1).select(\"weeks_worked_in_yr\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(weeks_worked_0, bins=50, label=\"Below $50K income\")\n",
    "plt.hist(weeks_worked_1, bins=50, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Histogram of Weeks worked in 1 year\")\n",
    "plt.xlabel('Weeks worked in 1 year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data distribution is very interesting. Most of the people either do not work (i.e. 0 week) or work for a whole year (i.e. 52 weeks). \n",
    "- A smaller number of people works part time during the year (lower frequency bars in the middle of the plot)\n",
    "- We can clearly see that most of people who have above \\$50K+ income/yr works for a whole year (orange part in week 52 bar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age vs Wage/hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(age_0, wage_per_hr_0, label=\"Below $50K income\")\n",
    "plt.scatter(age_1, wage_per_hr_1, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Scatter of Age vs Wage/hr\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Wage/hr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that most of the people who have above \\\\$50k+ income have wage/hr < \\\\$40/hr.\n",
    "- Above the threshold of \\\\$40 wage/hr, there are more people (\\*) with <50$K+ income than people with >\\\\$50K+ income. These people (\\*) probably worked part time instead of full-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Age vs Capital gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(age_0, capital_gain_0, label=\"Below $50K income\")\n",
    "plt.scatter(age_1, capital_gain_1, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Scatter of Age vs Capital gain\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Capital gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see most of the people have capital gain below \\\\$40K. Capital gain below \\\\$40K does not necessarily mean that your income will be lower than \\\\$50K, but really high capital gain as we can see (around \\\\$100K) will result in >\\\\$50K+ income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age vs Capital loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(age_0, capital_loss_0, label=\"Below $50K income\")\n",
    "plt.scatter(age_1, capital_loss_1, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Scatter of Age vs Capital loss\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Capital loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that almost all people with above $50K+ income have capital loss lower than \\\\$3000.\n",
    "- Intuitively, people who have a high capital loss, i.e. above \\\\$3K, tend to have below \\\\$50K+ income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age vs stock dividends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(age_0, stock_0, label=\"Below $50K income\")\n",
    "plt.scatter(age_1, stock_1, label=\"Above $50K income\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Scatter of Age vs Stock dividends\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Stock dividends')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below the threshold of \\\\$40K stock dividends, there are both type of income, either above or below \\\\$50K.\n",
    "- Above the threshold of \\\\$40K stock dividends, there are only people with income above $50K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal columns\n",
    "We will mainly use stacked bar plot labeled by income for nominal columns to visualize our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class of worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "worker_class_count = spark_ds.groupBy('class_of_worker').agg(\n",
    "    count(when((col(\"income\") == 0), True)).alias(\"count0\"),\n",
    "    count(when((col(\"income\") == 1), True)).alias(\"count1\")\n",
    ")\n",
    "\n",
    "# count rows labeled 0\n",
    "worker_class_0_count = [row['count0'] for row in worker_class_count.collect()]\n",
    "# count rows label 1\n",
    "worker_class_1_count = [row['count1'] for row in worker_class_count.collect()]\n",
    "# unique labels\n",
    "worker_class_labels = [row['class_of_worker'] for row in worker_class_count.collect()]\n",
    "index = np.arange(len(worker_class_labels))\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(index, worker_class_0_count, label=\"Below $50K income\")\n",
    "plt.bar(index, worker_class_1_count, label=\"Above $50K income\")\n",
    "plt.xticks(index, worker_class_labels, rotation=20)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Class of worker bar chart\")\n",
    "plt.xlabel('Class of worker')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems like the value \"Not in universe\" has the highest frequency in this column. Let's ignore that for now.\n",
    "- Most people have \"Private\" class of worker, and the number of people with > \\\\$50k+ income mostly come from this category as well (the highest orange part in Private bar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "education_count = spark_ds.groupBy('education').agg(\n",
    "    count(when((col(\"income\") == 0), True)).alias(\"count0\"),\n",
    "    count(when((col(\"income\") == 1), True)).alias(\"count1\")\n",
    ")\n",
    "\n",
    "# count rows labeled 0\n",
    "education_0_count = [row['count0'] for row in education_count.collect()]\n",
    "# count rows label 1\n",
    "education_1_count = [row['count1'] for row in education_count.collect()]\n",
    "# unique labels\n",
    "education_labels = [row['education'] for row in education_count.collect()]\n",
    "index = np.arange(len(education_labels))\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.bar(index, education_0_count, label=\"Below $50K income\")\n",
    "plt.bar(index, education_1_count, label=\"Above $50K income\")\n",
    "plt.xticks(index, education_labels, rotation=80)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Education bar chart\")\n",
    "plt.xlabel('Education')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the people taking the survey are High school graduate or Children.\n",
    "- All children have income level 0 (below \\\\$50K)\n",
    "- Bachelors degree have the highest proportion of people with income level 1 (above \\\\$50K). This is followed by \"Some college but no degree\", \"High school graduate\" and \"Masters degree\" with slightly less proportion of income level 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marital status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "marital_status_count = spark_ds.groupBy('marital_status').agg(\n",
    "    count(when((col(\"income\") == 0), True)).alias(\"count0\"),\n",
    "    count(when((col(\"income\") == 1), True)).alias(\"count1\")\n",
    ")\n",
    "\n",
    "# count rows labeled 0\n",
    "marital_status_0_count = [row['count0'] for row in marital_status_count.collect()]\n",
    "# count rows label 1\n",
    "marital_status_1_count = [row['count1'] for row in marital_status_count.collect()]\n",
    "# unique labels\n",
    "marital_status_labels = [row['marital_status'] for row in marital_status_count.collect()]\n",
    "index = np.arange(len(marital_status_labels))\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.bar(index, marital_status_0_count, label=\"Below $50K income\")\n",
    "plt.bar(index, marital_status_1_count, label=\"Above $50K income\")\n",
    "plt.xticks(index, marital_status_labels, rotation=20)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Marital status bar chart\")\n",
    "plt.xlabel('Marital status')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the people are \"Never married\", followed by \"Married-Civillian spouse present\".\n",
    "- \"Married-Civillian spouse present\" has the highest proportion of people with income level 1 (highest orange part)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "industry_count = spark_ds.groupBy('major_industry_code').agg(\n",
    "    count(when((col(\"income\") == 0), True)).alias(\"count0\"),\n",
    "    count(when((col(\"income\") == 1), True)).alias(\"count1\")\n",
    ")\n",
    "\n",
    "# count rows labeled 0\n",
    "industry_0_count = [row['count0'] for row in industry_count.collect()]\n",
    "# count rows label 1\n",
    "industry_1_count = [row['count1'] for row in industry_count.collect()]\n",
    "# unique labels\n",
    "industry_labels = [row['major_industry_code'] for row in industry_count.collect()]\n",
    "index = np.arange(len(industry_labels))\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.bar(index, industry_0_count, label=\"Below $50K income\")\n",
    "plt.bar(index, industry_1_count, label=\"Above $50K income\")\n",
    "plt.xticks(index, industry_labels, rotation=80)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Industry bar chart\")\n",
    "plt.xlabel('Industry')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the people taking the survey are \"Not in universe or Children\".\n",
    "- The most popular industry is Retail trade with >20K people.\n",
    "- The industry which has the most proportion of people with income level 1 is \"Manufacturing-durable goods\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the data\n",
    "sex_count = spark_ds.groupBy('sex').agg(\n",
    "    count(when((col(\"income\") == 0), True)).alias(\"count0\"),\n",
    "    count(when((col(\"income\") == 1), True)).alias(\"count1\")\n",
    ")\n",
    "\n",
    "# count rows labeled 0\n",
    "sex_0_count = [row['count0'] for row in sex_count.collect()]\n",
    "# count rows label 1\n",
    "sex_1_count = [row['count1'] for row in sex_count.collect()]\n",
    "# unique labels\n",
    "sex_labels = [row['sex'] for row in sex_count.collect()]\n",
    "index = np.arange(len(sex_labels))\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.bar(index, sex_0_count, label=\"Below $50K income\")\n",
    "plt.bar(index, sex_1_count, label=\"Above $50K income\")\n",
    "plt.xticks(index, sex_labels, rotation=80)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Sex chart\")\n",
    "plt.xlabel('Sex')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are slightly more females than males taking the survey.\n",
    "- Males have a higher proportion of people with income level 1 than female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark MLlib process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing pipelines\n",
    "The preprocessing pipeline will make sure that the features are converted correctly to be put into the ML classifier. This will include:\n",
    "1. For numerical columns:  \n",
    "    1.1. Use **VectorAssembler** to combine numeric columns into vector column \"numericFeatures\".  \n",
    "    1.2. Use **StandardScaler** to standardize (z-score) the numeric features.  \n",
    "2. For categorical columns:  \n",
    "    2.1. First, use **StringIndexer** to convert categorical columns to be numeric.  \n",
    "    2.2. Second, use **OneHotEncoderEstimator** to one hot encode the output of (2.1).  \n",
    "3. Use **VectorAssembler** to combine all columns we have to a single vector column called \"features\". This will be the input to our ML classifier later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "# 1. Numerical columns\n",
    "numeric_assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"numericFeatures\")\n",
    "scaler = StandardScaler(inputCol=\"numericFeatures\", outputCol=\"scaledNumericFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "# 2. Categorical columns\n",
    "# 2.1. StringIndexer\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=\"{}_indexed\".format(c))\n",
    "                 for c in nominal_cols]\n",
    "\n",
    "# 2.1. One hot encode\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(),\n",
    "             outputCol=\"{}_encoded\".format(indexer.getOutputCol()))\n",
    "             for indexer in indexers]\n",
    "\n",
    "# 3. Combine into features vector column\n",
    "final_assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\n",
    "                            + [scaler.getOutputCol()], outputCol=\"features\")\n",
    "\n",
    "# Execute pipeline on original dataset\n",
    "preprocessing_pipeline = Pipeline(stages=[numeric_assembler, scaler] + indexers + encoders + [final_assembler])\n",
    "transformed_ds = preprocessing_pipeline.fit(spark_ds).transform(spark_ds)\n",
    "\n",
    "# Select only features and label (income) column\n",
    "transformed_ds = transformed_ds.select(\"features\", \"income\")\n",
    "transformed_ds.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test set\n",
    "Originally the dataset has provided us with train and test file but we combine them to perform data exploration analysis + preprocessing. Now we will split them like originally provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "# Add a helper index column\n",
    "tranformed_ds = transformed_ds.withColumn('index', f.monotonically_increasing_id())\n",
    "\n",
    "# sort ascending and take first TRAIN_SIZE rows\n",
    "spark_train = tranformed_ds.sort('index').limit(TRAIN_SIZE).select(\"features\", \"income\")\n",
    "\n",
    "# sort descending and TEST_SIZE rows\n",
    "spark_test = tranformed_ds.sort('index', ascending=False).limit(TEST_SIZE).select(\"features\", \"income\")\n",
    "\n",
    "# Verify size\n",
    "print(\"Train size:\", spark_train.count())\n",
    "print(\"Test size:\", spark_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of label in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of label in train and test set\n",
    "print(\"Label distribution in train set:\")\n",
    "spark_train.groupBy(\"income\")\\\n",
    "                .count()\\\n",
    "                .withColumn(\"count\", col(\"count\") / spark_train.count() * 100)\\\n",
    "                .orderBy(desc(\"count\")).show()\n",
    "\n",
    "print(\"Label distribution in test set\")\n",
    "spark_test.groupBy(\"income\")\\\n",
    "                .count()\\\n",
    "                .withColumn(\"count\", col(\"count\") / spark_test.count() * 100)\\\n",
    "                .orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Models and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Setup model + train-validation-split hyperparameters tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"income\", featuresCol=\"features\")\n",
    "\n",
    "# Tuning evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"income\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Tuning parameters grid for logistic regression model\n",
    "lr_param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01, 0.1])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .addGrid(lr.maxIter, [10, 100])\\\n",
    "    .build()\n",
    "\n",
    "# Tuning train-validation-split settings\n",
    "lr_tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=lr_param_grid,\n",
    "                           evaluator=evaluator,\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8,\n",
    "                           parallelism=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Train model and predict on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train-validation-split on train set and get the best model\n",
    "lr_tvs_model = lr_tvs.fit(spark_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "lr_predictions = lr_tvs_model.transform(spark_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Best model found:\n",
    "lr_best = lr_tvs_model.bestModel\n",
    "\n",
    "print(\"Logistic Regression model summary:\")\n",
    "# Intercept\n",
    "print(\"Intercept:\", lr_best.intercept)\n",
    "# Coefficients\n",
    "print(\"Coefficients:\")\n",
    "coefficients = lr_best.coefficients\n",
    "coefficients = [(float(c),) for c in coefficients]  # convert numpy type to float, and to tuple\n",
    "coefficients_df = sqlContext.createDataFrame(coefficients, [\"Feature Coefficients\"])\n",
    "coefficients_df.show()\n",
    "\n",
    "# Regularization coefficient\n",
    "print(\"Regularization coefficient:\", lr_best._java_obj.getRegParam())\n",
    "# Elastic net coefficient\n",
    "print(\"Elastic net coefficient:\", lr_best._java_obj.getElasticNetParam())\n",
    "# Max iteration\n",
    "print(\"Max iterations:\", lr_best._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Get overall statistics (i.e. accuracy, precision, recall and F1-score)\n",
    "preds_labels_rdd = lr_predictions.rdd.map(lambda x: (float(x.prediction), float(x.income)))\n",
    "lr_metrics = MulticlassMetrics(preds_labels_rdd)\n",
    "\n",
    "# Overall statistics\n",
    "lr_accuracy = lr_metrics.accuracy\n",
    "# We are interested in the score of precision, recall and F1-score for positive class (few number of samples)\n",
    "lr_precision = lr_metrics.precision(1.0)\n",
    "lr_recall = lr_metrics.recall(1.0)\n",
    "lr_f1Score = lr_metrics.fMeasure(1.0)\n",
    "print(\"Logistic Regression summary stats:\")\n",
    "print(\"Accuracy = {}\".format(lr_accuracy))\n",
    "print(\"Precision = {}\".format(lr_precision))\n",
    "print(\"Recall = {}\".format(lr_recall))\n",
    "print(\"F1 Score = {}\".format(lr_f1Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Setup model + train-validation-split hyperparameters tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Create initial DecisionTree model\n",
    "dt = DecisionTreeClassifier(labelCol=\"income\", \n",
    "                            featuresCol=\"features\", \n",
    "                            minInstancesPerNode=20)\n",
    "\n",
    "# Tuning evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"income\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Tuning parameters grid for decision tree model\n",
    "dt_param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(dt.maxDepth, [15, 20, 30])\\\n",
    "    .addGrid(dt.maxBins, [20, 60])\\\n",
    "    .addGrid(dt.impurity, ['gini', 'entropy'])\\\n",
    "    .build()\n",
    "\n",
    "# Tuning train-validation-split settings\n",
    "dt_tvs = TrainValidationSplit(estimator=dt,\n",
    "                           estimatorParamMaps=dt_param_grid,\n",
    "                           evaluator=evaluator,\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8,\n",
    "                           parallelism=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Train model and predict on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train-validation-split on train set and get the best model\n",
    "dt_tvs_model = dt_tvs.fit(spark_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "dt_predictions = dt_tvs_model.transform(spark_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model found:\n",
    "dt_best = dt_tvs_model.bestModel\n",
    "\n",
    "print(\"Decision Tree model summary:\")\n",
    "print(\"Depth:\", dt_best.depth)\n",
    "print(\"Number of nodes:\", dt_best.numNodes)\n",
    "print(\"Impurity criteria:\", dt_best._java_obj.getImpurity())\n",
    "print(\"Max bins:\", dt_best._java_obj.getMaxBins())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get overall statistics (i.e. accuracy, precision, recall and F1-score)\n",
    "preds_labels_rdd = dt_predictions.rdd.map(lambda x: (float(x.prediction), float(x.income)))\n",
    "dt_metrics = MulticlassMetrics(preds_labels_rdd)\n",
    "\n",
    "# Overall statistics\n",
    "dt_accuracy = dt_metrics.accuracy\n",
    "# We are interested in the score of precision, recall and F1-score for positive class (few number of samples)\n",
    "dt_precision = dt_metrics.precision(1.0)\n",
    "dt_recall = dt_metrics.recall(1.0)\n",
    "dt_f1Score = dt_metrics.fMeasure(1.0)\n",
    "print(\"Decision Tree summary stats:\")\n",
    "print(\"Accuracy = {}\".format(dt_accuracy))\n",
    "print(\"Precision = {}\".format(dt_precision))\n",
    "print(\"Recall = {}\".format(dt_recall))\n",
    "print(\"F1 Score = {}\".format(dt_f1Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Setup model + train-validation-split hyperparameters tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Create initial RandomForest model\n",
    "rf = RandomForestClassifier(labelCol=\"income\", \n",
    "                            featuresCol=\"features\",\n",
    "                            impurity=\"entropy\",                          \n",
    "                            maxDepth=15,\n",
    "                            maxBins=60,\n",
    "                            minInstancesPerNode=20)\n",
    "\n",
    "# Tuning evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"income\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Tuning parameters grid for random forest model\n",
    "rf_param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30])\\\n",
    "    .build()\n",
    "\n",
    "# Tuning train-validation-split settings\n",
    "rf_tvs = TrainValidationSplit(estimator=rf,\n",
    "                           estimatorParamMaps=rf_param_grid,\n",
    "                           evaluator=evaluator,\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8,\n",
    "                           parallelism=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Train model and predict on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train-validation-split on train set and get the best model\n",
    "rf_tvs_model = rf_tvs.fit(spark_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "rf_predictions = rf_tvs_model.transform(spark_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model found:\n",
    "rf_best = rf_tvs_model.bestModel\n",
    "\n",
    "print(\"Random Forest model summary:\")\n",
    "print(\"Number of trees:\", rf_best._java_obj.getNumTrees())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get overall statistics (i.e. accuracy, precision, recall and F1-score)\n",
    "preds_labels_rdd = rf_predictions.rdd.map(lambda x: (float(x.prediction), float(x.income)))\n",
    "rf_metrics = MulticlassMetrics(preds_labels_rdd)\n",
    "\n",
    "# Overall statistics\n",
    "rf_accuracy = rf_metrics.accuracy\n",
    "# We are interested in the score of precision, recall and F1-score for positive class (few number of samples)\n",
    "rf_precision = rf_metrics.precision(1.0)\n",
    "rf_recall = rf_metrics.recall(1.0)\n",
    "rf_f1Score = rf_metrics.fMeasure(1.0)\n",
    "print(\"Random Forest summary stats:\")\n",
    "print(\"Accuracy = {}\".format(rf_accuracy))\n",
    "print(\"Precision = {}\".format(rf_precision))\n",
    "print(\"Recall = {}\".format(rf_recall))\n",
    "print(\"F1 Score = {}\".format(rf_f1Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark MLlib result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 3\n",
    "index = np.arange(num_models)\n",
    "width = 0.15 # width of bar\n",
    "\n",
    "# trained models' results\n",
    "spark_accuracy = [lr_accuracy, dt_accuracy, rf_accuracy]\n",
    "spark_precision = [lr_precision, dt_precision, rf_precision]\n",
    "spark_recall = [lr_recall, dt_recall, rf_recall]\n",
    "spark_f1 = [lr_f1Score, dt_f1Score, rf_f1Score]\n",
    "\n",
    "# side-by-side bar plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.bar(index, spark_accuracy, width, label=\"Accuracy\")\n",
    "plt.bar(index + width, spark_precision, width, label=\"Precision\")\n",
    "plt.bar(index + width * 2, spark_recall, width, label=\"Recall\")\n",
    "plt.bar(index + width * 3, spark_f1, width, label=\"F1 Score\")\n",
    "\n",
    "# Additional info\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Pyspark MLlib Result summary\")\n",
    "plt.xticks(index + width * 1.5, ('Logistic Regression', 'Decision Tree', 'Random Forest'))\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "# Pandas dataframe\n",
    "pd_dataset = spark_ds.toPandas()\n",
    "# To get train and test set\n",
    "pd_train = pd_dataset.iloc[:TRAIN_SIZE, :]\n",
    "pd_test = pd_dataset.iloc[TRAIN_SIZE:, :]\n",
    "\n",
    "pd_train.shape\n",
    "pd_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that there are no missing values :)\n",
    "pd_dataset.isnull().sum().sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
